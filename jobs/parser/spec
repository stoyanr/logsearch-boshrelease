---
name: parser
packages:
- logstash
- logsearch-config
- java8
templates:
  bin/parser_ctl: bin/parser_ctl
  bin/monit_debugger: bin/monit_debugger
  data/properties.sh.erb: data/properties.sh
  helpers/ctl_setup.sh: helpers/ctl_setup.sh
  helpers/ctl_utils.sh: helpers/ctl_utils.sh
  config/input_and_output.conf.erb: config/input_and_output.conf
  config/filters_pre.conf.erb: config/filters_pre.conf
  config/filters_post.conf.erb: config/filters_post.conf
  config/filters_override.conf.erb: config/filters_override.conf
  logsearch/logs.yml: logsearch/logs.yml
properties:
  logstash.metadata_level:
    description: "Whether to include additional metadata throughout the event lifecycle. NONE = disabled, DEBUG = fully enabled"
    default: "NONE"

  logstash_parser.debug:
    description: Debug level logging
    default: false
  logstash_parser.message_max_size:
    description: "Maximum log message length.  Anything larger is truncated (TODO: move this to ingestor?)"
    default: 1048576
  logstash_parser.filters:
    description: "The configuration to embed into the logstash filters section. Can either be a set of parsing rules as a string or a list of hashes in the form of [{name: path_to_parsing_rules.conf}]"
    default: ''
  logstash_parser.inputs:
    description: |
      A list of input plugins, with a hash of options for each of them. Please refer to example below.
    example:
      inputs:
        - plugin: rabbitmq
          options:
            host: 192.168.1.1
            user: logsearch
            password: c1oudbunny
    default: [ { plugin: "redis", options : {} } ]
  logstash_parser.outputs:
    description: "The configuration to embed into the logstash outputs section"
  logstash_parser.workers:
    description: "The number of worker threads that logstash should use (default: auto = one per CPU)"
    default: auto
  logstash_parser.idle_flush_time:
    description: "How frequently to flush events if the output queue is not full."
  logstash_parser.elasticsearch_document_id:
    description: "Use a specific, dynamic ID rather than an auto-generated identifier."
    default: ~
  logstash_parser.elasticsearch_index:
    description: "The specific, dynamic index name to write events to."
    default: "logstash-%{+YYYY.MM.dd}"
  logstash_parser.elasticsearch_index_type:
    description: "The specific, dynamic index type name to write events to."
    default: "%{@type}"
  logstash_parser.timecop.reject_greater_than_hours:
    description: "Logs with timestamps greater than this many hours in the future won't be parsed and will get tagged with fail/timecop"
    default: 1
  logstash_parser.timecop.reject_less_than_hours:
    description: "Logs with timestamps less than this many hours in the past won't be parsed and will get tagged with fail/timecop"
    default: 24
  logstash_parser.excluded_filters:
    description: "Default filters that should not be added to the final logstash.conf"
    example: [logstash-filters-default, if_it_looks_like_json, timecop]
    default: []

  logstash.output.elasticsearch.data_hosts:
    description: The list of elasticsearch data node IPs
    default: [127.0.0.1]
  logstash.output.elasticsearch.flush_size:
    description: Controls how many logs will be buffered and sent to Elasticsearch for bulk indexing
    default: 500

  redis.host:
    description: Redis host of queue
  redis.port:
    description: Redis port of queue
    default: 6379
  redis.key:
    description: Name of queue to pull messages from
    default: logstash
